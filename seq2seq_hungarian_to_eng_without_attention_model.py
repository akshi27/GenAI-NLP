# -*- coding: utf-8 -*-
"""Seq2Seq_Hungarian_to_Eng_without_attention_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Owe2dn_ps8SznZkO7VZxP4xjhUHuB_X0

## Recurrence-based Seq2Seq Neural Machine Translation without Attention

Built a model with two LSTMs (one encoder and one decoder) to translate Hungarian to English. The dataset used comes from Tatoeba, a collection of sentence translations in a variety of languages sourced from volunteers:
https://tatoeba.org/en
"""

!pip install numpy

!pip install pandas

!pip install tensorflow

import io
import json
import numpy as np
import pandas as pd
import random
import re
import tensorflow as tf
import unicodedata

from google.colab import files
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## Preprocessing the training dataset"""

!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt  # to download the training data set

with open('hun_eng_pairs_train.txt') as file:
  train = [line.rstrip() for line in file]

len(train)

# Separating the input (Hungarian) and target (English) sentences into 2 separate lists

SEPARATOR = '<sep>'
train_input, train_target = map(list, zip(*[pair.split(SEPARATOR) for pair in train]))

print(train_input[:10])
print(train_target[:10])

# Since Hungarian is a source language that uses accented characters, Unicode normalization is important here
# Though these characters look the same to us, they'll be treated differently by a NLP model
# Hence, a Unicode normalization function is needed where it normalizes any accented characters into the same set of Unicode, and then replaces them with their ASCII equivalents

def normalize_unicode(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn')

# A tokenizer won't filter out punctuation and treat them as seperate tokens
# But since we're building a word-based translation model where we would still want to keep punctuation and treat them as separate tokens,
# we'll insert a space between any relevant punctuation and the characters around them
# As such, the tokenizer will be able to output punctuation marks as separate tokens

# Defining a function that does both inserting spaces between punctuation marks and unicode normalization

def preprocess_sentence(s):
  s = normalize_unicode(s)
  s = re.sub(r"([?.!,Â¿])", r" \1 ", s)
  s = re.sub(r'[" "]+', " ", s)
  s = s.strip()
  return s

# Preprocessing both the source and target sentences

train_preprocessed_input = [preprocess_sentence(s) for s in train_input]
train_preprocessed_target = [preprocess_sentence(s) for s in train_target]

train_preprocessed_input[:5]

train_preprocessed_target[:5]

# Using Teacher Forcing with the translation model
# So we're adding a start-of-sentence tag (<sos>) and a end-of-sentence tag (<eos>) at the beginning and end of each target sentence respectively

def tag_target_sentences(sentences):
  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)
  return list(tagged_sentences)

train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)

train_tagged_preprocessed_target[:5]

# When we're testing the model, it might encounter words that it hadn't seen during the initial fit on the training data
# Hence, we're including an out-of-vocabulary token (<unk>) in the tokenizer initialization so that the translation system can cope up with it

# Tokenizer for the Hungarian input sentences

source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')  # filtering out the unwanted punctuations, keeping !,? and periods still intact
source_tokenizer.fit_on_texts(train_preprocessed_input)
source_tokenizer.get_config()

source_vocab_size = len(source_tokenizer.word_index) + 1  # the +1 here accounts for the padding token
print(source_vocab_size)

# Tokenizer for the English target sentences

target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')
target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)
target_tokenizer.get_config()

target_vocab_size = len(target_tokenizer.word_index) + 1
print(target_vocab_size)

# A smaller English vocab set maps to a larger Hungarian vocab set

# Vectorizing the input and target sentences

train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)

print(train_encoder_inputs[:5])
print(source_tokenizer.sequences_to_texts(train_encoder_inputs[:3]))

# For teacher forcing, we are creating two copies of each vectorized target sentence, a 'decoder_input sentence' and a 'decoder_target sentence'
# The 'decoder_input sentence' will include every token except the last and the 'decoder_target sentence' will include every token except the first

def generate_decoder_inputs_targets(sentences, tokenizer):
  seqs = tokenizer.texts_to_sequences(sentences)
  decoder_inputs = [s[:-1] for s in seqs] # Drop the last token in the sentence.
  decoder_targets = [s[1:] for s in seqs] # Drop the first token in the sentence.

  return decoder_inputs, decoder_targets

train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target,
                                                                              target_tokenizer)   # both inputs and targets are in English

# Each token of a decoder_input sentence will be fed to the decoder as the next expected token
# Each token of a decoder_target sentence will be used to calculate the loss against the decoder's actual output

print(train_decoder_inputs[0], train_decoder_targets[0])

print(target_tokenizer.sequences_to_texts(train_decoder_inputs[:1]),   # decoder_input_sentence (to be fed to the decoder)
      target_tokenizer.sequences_to_texts(train_decoder_targets[:1]))  # decoder_target_sentence (used to calculate the loss against the decoder's output)

max_encoding_len = len(max(train_encoder_inputs, key=len))
max_encoding_len

max_decoding_len = len(max(train_decoder_inputs, key=len))
max_decoding_len

# Padding each sentence in the collection to the same length as the longest sentence in the collection

padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')
padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')
padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')

print(padded_train_encoder_inputs[0])
print(padded_train_decoder_inputs[0])
print(padded_train_decoder_targets[0])

# when a padded sequence is converted back to text, padding is replaced with the out-of-vocabulary <unk> token

target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[0]])

"""## Preprocessing the validation dataset"""

# To download the validation dataset

!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_val.txt

with open('hun_eng_pairs_val.txt') as file:
  val = [line.rstrip() for line in file]

# Creating a preprocessing function

def process_dataset(dataset):

  input, output = map(list, zip(*[pair.split(SEPARATOR) for pair in dataset]))  # Splits the Hungarian and English sentences into separate lists

  # Unicode normalization and inserting spaces around punctuation
  preprocessed_input = [preprocess_sentence(s) for s in input]
  preprocessed_output = [preprocess_sentence(s) for s in output]

  tagged_preprocessed_output = tag_target_sentences(preprocessed_output)  # Tagging target sentences with <sos> and <eos> tokens for teacher forcing

  encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)  # Vectorizing encoder source sentences

  # Vectorize and create decoder input and target sentences.
  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output,
                                                                    target_tokenizer)

  # Pad all collections.
  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')
  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')
  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')

  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets

# Preprocessing the validation dataset

padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(val)

"""## Building the model"""

embedding_dim = 128
hidden_dim = 256
default_dropout=0.2
batch_size = 32
epochs = 30

# Initializing a Functional API instead of a Sequential API as the model has 2 inputs; one of the encoder and the other for the decoder

# 1) The encoder receives the source sentences (Hungarian) and generates the initial state inputs for the decoder.
# 2) The decoder receives the decoder target input sentences (English) for teacher forcing.

"""### Encoder"""

# The initial encoder input layer takes in padded sequences
encoder_inputs = layers.Input(shape=[None], name='encoder_inputs')

# Creating an embedding layer
encoder_embeddings = layers.Embedding(source_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='encoder_embeddings')

# Passing the input layer's output to the embedding layer creates a link between the two and will output a sequence of embeddings
encoder_embedding_output = encoder_embeddings(encoder_inputs)

# Creating a LSTM layer
# Since we're not using the ATTENTION mechanism in this model, we're setting the return_state to True and return_sequences to False
encoder_lstm = layers.LSTM(hidden_dim,
                           return_state=True,
                           dropout=default_dropout,
                           name='encoder_lstm')

# Passing the embedding layer's output to the LSTM layer creates another link.
# Since the return_sequences is False, encoder_outputs and state_h (encoder hidden states from each time step) are the same
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)

# The final hidden and cell states from the encoder will be the the initial states for the decoder
encoder_states = (state_h, state_c)

"""### Decoder"""

decoder_inputs = layers.Input(shape=[None], name='decoder_inputs')


decoder_embeddings = layers.Embedding(target_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='decoder_embeddings')


decoder_embedding_output = decoder_embeddings(decoder_inputs)

# The LSTM has return_sequences set to True since we'll need the hidden state outputted at each timestep
decoder_lstm = layers.LSTM(hidden_dim,
                           return_sequences=True,
                           return_state=True,
                           dropout=default_dropout,
                           name='decoder_lstm')


# The decoder's initial state is set to the encoder's final output states as we are feeding in that too
# Since we don't need the decoder's final hidden output and cell states, those are set to _
decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)

# Creating a softmax layer in the end to create a probability distribution for the output word
decoder_dense = layers.Dense(target_vocab_size, activation='softmax', name='decoder_dense')

# The probability distribution for the output word
y_proba = decoder_dense(decoder_outputs)

model = tf.keras.Model([encoder_inputs, decoder_inputs], y_proba, name='hun_eng_seq2seq_nmt_no_attention')

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  metrics='sparse_categorical_accuracy')
model.summary()

# Visualizing the model

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='hun_eng_seq2seq_nmt_no_attention.png', show_shapes=True, show_layer_names=True)

print('encoder_inputs layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len)))
print()
print('encoder_embeddings layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len, embedding_dim)))
print()
print('encoder_lstm layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len, embedding_dim), [(batch_size, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))
print()
print()
print('decoder_inputs layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len)))
print()
print('decoder_embeddings layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len, embedding_dim)))
print()
print('decoder_lstm layer\n input dimension {}\n output dimension: {}'.format([(batch_size, max_decoding_len, embedding_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)], [(batch_size, max_decoding_len, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))
print()
print('decoder_dense layer(softmax)\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len, hidden_dim), (batch_size, max_decoding_len, target_vocab_size)))

# Saving to a folder
filepath="./HunEngNMTNoAttention/training1/cp.ckpt"

# Creating a callback using model checkpoints to save the weights after every epoch so that if something goes wrong in the system during training, the last set of weights can be reloaded from the checkpoint and resume the training
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,
                                                 save_weights_only=True,
                                                 verbose=1)

es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)  # early stopping to prevent overfitting

history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_targets,
                     batch_size=batch_size,
                     epochs=epochs,
                     validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),
                     callbacks=[cp_callback, es_callback])

"""## Testing on the test dataset"""

# Retrieving the tokenizers
!wget https://github.com/futuremojo/nlp-demystified/raw/main/models/nmt_no_attention/hun_eng_s2s_nmt_no_attention_tokenizers.zip

!unzip -o hun_eng_s2s_nmt_no_attention_tokenizers.zip

# Loading the tokenizers

with open('source_tokenizer.json') as f:
    data = json.load(f)
    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)

with open('target_tokenizer.json') as f:
    data = json.load(f)
    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)

# Retrieving the model
!wget https://github.com/futuremojo/nlp-demystified/raw/main/models/nmt_no_attention/hun_eng_s2s_nmt_no_attention_model.zip

!unzip -o hun_eng_s2s_nmt_no_attention_model.zip

# Loading  the model
model = tf.keras.models.load_model('hun_eng_s2s_nmt_no_attention')

"""### Preprocessing the test dataset"""

# Retrieving the test dataset
!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_test.txt

with open('hun_eng_pairs_test.txt') as file:
  test = [line.rstrip() for line in file]

test[:3]

# Preprocessing the test dataset
padded_test_encoder_inputs, padded_test_decoder_inputs, padded_test_decoder_targets = process_dataset(test)

# Evaluating the model on the test set
model.evaluate([padded_test_encoder_inputs, padded_test_decoder_inputs], padded_test_decoder_targets)

"""Taking the trained layers from the training model and creating separate, stand-alone encoder and decoder models will give much greater control over how the output is created."""

# These are the layers of the trained model
[layer.name for layer in model.layers]

"""### Creating a stand-alone encoder"""

# Calling tf.keras.Model to create a stand-alone encoder with encoder_inputs as the input and encoder_states as the output

encoder_inputs = model.get_layer('encoder_inputs').input

encoder_embedding_layer = model.get_layer('encoder_embeddings')
encoder_embeddings = encoder_embedding_layer(encoder_inputs)

encoder_lstm = model.get_layer('encoder_lstm')

_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)

encoder_states = [encoder_state_h, encoder_state_c]

# The stand-alone encoder model
encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states) # encoder_inputs is the input to the encoder and encoder_states is the expected output

plot_model(encoder_model_no_attention, to_file='encoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)

"""### Creating a stand-alone decoder"""

# The encoder's output (its final states) is used as the decoder's initial state
# At each time step, the decoder's state outputs are taken and fed to the next time step
# A probability distribution is then given for the current output, and new hidden and cell states

decoder_inputs = model.get_layer('decoder_inputs').input

decoder_embedding_layer = model.get_layer('decoder_embeddings')
decoder_embeddings = decoder_embedding_layer(decoder_inputs)

# Inputs to represent the decoder's LSTM hidden and cell states
decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')
decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')
decoder_input_states = [decoder_input_state_h, decoder_input_state_c]

decoder_lstm = model.get_layer('decoder_lstm')

decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(
    decoder_embeddings, initial_state=decoder_input_states
)

# Update hidden and cell states for the next time step
decoder_output_states = [decoder_output_state_h, decoder_output_state_c]

decoder_dense = model.get_layer('decoder_dense')
y_proba = decoder_dense(decoder_sequence_outputs)

decoder_model_no_attention = tf.keras.Model(
    [decoder_inputs] + decoder_input_states,
    [y_proba] + decoder_output_states
)

plot_model(decoder_model_no_attention, to_file='decoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)

"""## The Test model"""

def translate_without_attention(sentence: str,
                                source_tokenizer, encoder,
                                target_tokenizer, decoder,
                                max_translated_len = 30):

  # Vectorizing the source sentence and run it through the encoder
  input_seq = source_tokenizer.texts_to_sequences([sentence])

  # Getting the tokenized sentence to see if there are any unknown tokens
  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)

  states = encoder.predict(input_seq)

  current_word = '<sos>'
  decoded_sentence = []

  while len(decoded_sentence) < max_translated_len:

    # Setting the next input word for the decoder
    target_seq = np.zeros((1,1))
    target_seq[0, 0] = target_tokenizer.word_index[current_word]

    # Determining the next word
    target_y_proba, h, c = decoder.predict([target_seq] + states)
    target_token_index = np.argmax(target_y_proba[0, -1, :])
    current_word = target_tokenizer.index_word[target_token_index]

    if (current_word == '<eos>'):
      break

    decoded_sentence.append(current_word)
    states = [h, c]

  return tokenized_sentence[0], ' '.join(decoded_sentence)

"""To test it out, I've sample a bunch of sentences from the test dataset and translate them."""

random.seed(1)
sentences = random.sample(test, 15)
sentences

def translate_sentences(sentences, translation_func, source_tokenizer, encoder,
                        target_tokenizer, decoder):
  translations = {'Tokenized Original': [], 'Reference': [], 'Translation': []}

  for s in sentences:
    source, target = s.split(SEPARATOR)
    source = preprocess_sentence(source)
    tokenized_sentence, translated = translation_func(source, source_tokenizer, encoder,
                                                      target_tokenizer, decoder)

    translations['Tokenized Original'].append(tokenized_sentence)
    translations['Reference'].append(target)
    translations['Translation'].append(translated)

  return translations

# Loading the results into a Pandas DataFrame
translations_no_attention = pd.DataFrame(translate_sentences(sentences, translate_without_attention,
                                                             source_tokenizer, encoder_model_no_attention,
                                                             target_tokenizer, decoder_model_no_attention))
translations_no_attention